---
title: "STA_141C_FP"
author: "Johnson Tian"
date: "2024-05-25"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(factoextra)
library(Rtsne)
library(cluster)
library(randomForest)
library(caret)
library(dbscan)
library(corrplot)
library(ggcorrplot)
library(neuralnet)
library(FactoMineR)
library(gridExtra)
library(dendextend)
```




```{r}
# Read data
car_data2 <- read.csv("~/Downloads/car_data2.csv")

# List of numerical features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# List of categorical features
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# List of binary features
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert'
)

# Convert binary features from "yes"/"no" to 1/0
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# Extract numerical features
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Extract categorical features
categorical_data <- car_data2 %>%
  select(all_of(categorical_features))

# Extract binary features
binary_data <- car_data2 %>%
  select(all_of(binary_features))

# Print extracted data

# Check the first few rows to verify changes
head(car_data2)

```

```{r}
# Read data
car_data2 <- read.csv("~/Downloads/car_data2.csv")

# List of binary features
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert'
)

# Convert binary features
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, as.numeric(x))))

# Extract binary features
binary_data <- car_data2 %>%
  select(all_of(binary_features))

# Ensure all features are numeric
binary_data <- as.data.frame(lapply(binary_data, as.numeric))

# Perform DBSCAN clustering
set.seed(123)
dbscan_result <- dbscan(binary_data, eps = 0.5, minPts = 5)

# View clustering results
print(dbscan_result)

# Add clustering results to the original data
car_data2$cluster <- dbscan_result$cluster

# Visualize clustering results (using PCA to reduce to 2D for visualization)
binary_data_pca <- prcomp(binary_data, center = TRUE, scale. = TRUE)
pca_data <- data.frame(binary_data_pca$x[,1:2], cluster = as.factor(dbscan_result$cluster))
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "DBSCAN Clustering on Binary Features (PCA Reduced)", x = "Principal Component 1", y = "Principal Component 2")

# Analyze insurance claim rate for each cluster
cluster_claim_analysis <- car_data2 %>%
  group_by(cluster) %>%
  summarise(
    claim_rate = mean(claim_status)
  )

print(cluster_claim_analysis)
```

```{r}
# Analyze feature distribution for each cluster
cluster_summary <- car_data2 %>%
  group_by(cluster) %>%
  summarise_all(mean, na.rm = TRUE)

print(cluster_summary)
```


```{r}
# Use clustering results as the dependent variable and binary features as independent variables
set.seed(123)
rf_model <- randomForest(as.factor(car_data2$claim_status) ~ ., data = binary_data)

# View feature importance
importance <- importance(rf_model)
var_importance <- data.frame(Feature = rownames(importance), Importance = importance[, 1])

# Sort by importance
var_importance <- var_importance[order(-var_importance$Importance), ]
print(var_importance)

# Visualize feature importance
ggplot(var_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance for Predicting Insurance Claims", x = "Feature", y = "Importance")
```


```{r}
# Read data
car_data2 <- read.csv("~/Downloads/car_data2.csv")

# List of categorical features
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# Calculate and display the distribution of each categorical variable
for (feature in categorical_features) {
  cat("Data Distribution for:", feature, "\n")
  print(table(car_data2[[feature]]))
  cat("\n")
}
```


```{r}
# List of categorical features
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# Generate frequency tables and bar plots
for (feature in categorical_features) {
  cat("Frequency Distribution for:", feature, "\n")
  print(table(car_data2[[feature]]))
  cat("\n")
  
  # Plot bar chart
  ggplot(car_data2, aes_string(x = feature)) +
    geom_bar() +
    labs(title = paste("Distribution of", feature), x = feature, y = "Count") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme_minimal()
}

# Generate cross-tabulations and stacked bar charts for categorical features with claim_status
for (feature in categorical_features) {
  cat("Cross-tabulation with claim_status for:", feature, "\n")
  print(table(car_data2[[feature]], car_data2$claim_status))
  cat("\n")
  
  # Plot stacked bar chart
  ggplot(car_data2, aes_string(x = feature, fill = "factor(claim_status)")) +
    geom_bar(position = "fill") +
    labs(title = paste("Distribution of", feature, "by Claim Status"), x = feature, y = "Proportion") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme_minimal()
}

# Generate cross-tabulations and heatmaps for pairs of categorical features
library(reshape2)

for (i in 1:(length(categorical_features) - 1)) {
  for (j in (i + 1):length(categorical_features)) {
    feature1 <- categorical_features[i]
    feature2 <- categorical_features[j]
    
    cat("Cross-tabulation between", feature1, "and", feature2, "\n")
    crosstab <- table(car_data2[[feature1]], car_data2[[feature2]])
    print(crosstab)
    cat("\n")
    
    # Plot heatmap
    crosstab_melt <- melt(crosstab)
    ggplot(crosstab_melt, aes_string(x = "Var1", y = "Var2", fill = "value")) +
      geom_tile() +
      labs(title = paste("Heatmap of", feature1, "and", feature2), x = feature1, y = feature2) +
      scale_fill_gradient(low = "white", high = "blue") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      theme_minimal()
  }
}

# Perform chi-squared tests of independence for pairs of categorical features
for (i in 1:(length(categorical_features) - 1)) {
  for (j in (i + 1):length(categorical_features)) {
    feature1 <- categorical_features[i]
    feature2 <- categorical_features[j]
    
    cat("Chi-squared test between", feature1, "and", feature2, "\n")
    crosstab <- table(car_data2[[feature1]], car_data2[[feature2]])
    chi_test <- chisq.test(crosstab)
    print(chi_test)
    cat("\n")
  }
}


```



```{r}
# Perform MCA
mca_result <- MCA(categorical_data, graph = TRUE)

# View MCA results
print(mca_result)
```



```{r}
# View feature contributions to the first two principal components
contrib_vars <- mca_result$var$contrib
print(contrib_vars)

# Visualize feature contributions to the first principal component
fviz_contrib(mca_result, choice = "var", axes = 1, top = 10)
# Visualize feature contributions to the second principal component
fviz_contrib(mca_result, choice = "var", axes = 2, top = 10)

# Visualize variable coordinates with color representing cos2 values
fviz_mca_var(mca_result, 
             col.var = "cos2", # Color by cos2 values
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)  # Avoid label overlap
```


```{r}
# List of categorical features
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# Extract categorical feature data
categorical_data <- car_data2 %>%
  select(all_of(categorical_features))

# Perform MCA
mca_result <- MCA(categorical_data, graph = FALSE)

# Visualize only the most contributing variables (e.g., top 20)
fviz_mca_var(mca_result, 
             title = "MCA - Variables (Top 20)", 
             label = "var", # Label type: "var" or "ind"
             select.var = list(contrib = 20), # Show only the top 20 contributing variables
             repel = TRUE)  # Avoid label overlap
```


```{r}
# Specify the categories to display
specific_categories <- c("Diesel", "Petrol", "CNG", "Automatic", "Manual")

# Visualize only the specified categories
fviz_mca_var(mca_result, 
             title = "MCA - Specific Variables", 
             label = "var", # Label type: "var" or "ind"
             select.var = list(name = specific_categories), # Display only specific categories
             repel = TRUE)  # Avoid label overlap


```




```{r}
# Read data
car_data2 <- read.csv("~/Downloads/car_data2.csv")

# List of numerical features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# Extract numerical feature data
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Standardize data
scaled_data <- scale(numeric_data)

# Randomly sample a portion of the data
set.seed(123)
sample_indices <- sample(1:nrow(scaled_data), size = 10000)
sample_data <- scaled_data[sample_indices, ]

# Determine the optimal number of clusters using the elbow method
fviz_nbclust(sample_data, kmeans, method = "wss")

# Determine the optimal number of clusters using the silhouette method
fviz_nbclust(sample_data, kmeans, method = "silhouette")

# Assume 4 clusters are chosen
set.seed(123)
kmeans_result <- kmeans(scaled_data, centers = 4, nstart = 25)

# View clustering results
print(kmeans_result)

# Visualize clustering results (using PCA to reduce to 2D for visualization)
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
pca_data <- data.frame(pca_result$x[,1:2], cluster = as.factor(kmeans_result$cluster))
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering (PCA Reduced)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

# Add clustering results to the original data
car_data2$cluster <- kmeans_result$cluster

# Analyze feature distribution for each cluster
cluster_summary <- car_data2 %>%
  group_by(cluster) %>%
  summarise_all(mean, na.rm = TRUE)

print(cluster_summary)

# Analyze insurance claim rate for each cluster
cluster_claim_analysis <- car_data2 %>%
  group_by(cluster) %>%
  summarise(
    claim_rate = mean(claim_status)
  )

print(cluster_claim_analysis)
```



```{r}
# Read data
car_data2 <- read.csv("~/Downloads/car_data2.csv")

# List of numerical features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# Extract numerical feature data
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Add target variable
numeric_data$claim_status <- car_data2$claim_status

# Calculate correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Plot correlation heatmap using corrplot package
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black", number.cex = 0.7)

# Plot correlation heatmap using ggcorrplot package
ggcorrplot(cor_matrix, 
           method = "square", 
           lab = TRUE, 
           lab_size = 3, 
           colors = c("blue", "white", "red"), 
           title = "Correlation Heatmap",
           tl.cex = 12, 
           tl.srt = 45)
```

```{r}
# Create a list of box plots
plots <- lapply(numeric_features, function(feature) {
  ggplot(car_data2, aes_string(y = feature)) + 
    geom_boxplot() + 
    labs(title = feature) +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
})

# Arrange box plots side by side
grid.arrange(grobs = plots, ncol = 4)



```



```{r}
# List of numerical features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# Extract numerical feature data
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Standardize data
numeric_data_scaled <- scale(numeric_data)

# Perform PCA analysis
pca_result <- prcomp(numeric_data_scaled, center = TRUE, scale. = TRUE)

# Print PCA results
summary(pca_result)

# Convert claim_status to a factor variable
car_data2$claim_status <- as.factor(car_data2$claim_status)

# Visualize PCA results
# Principal Component plot
fviz_pca_ind(pca_result,
             geom.ind = "point", # Show points for individuals
             pointshape = 21,
             pointsize = 2,
             fill.ind = car_data2$claim_status, # Color by claim status
             palette = c("#00AFBB", "#FC4E07"),
             addEllipses = TRUE, # Add confidence ellipses
             label = "var",
             col.var = "black",
             repel = TRUE) + 
  theme_minimal()

# Variable plot
fviz_pca_var(pca_result,
             col.var = "contrib", # Color by contribution
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) + 
  theme_minimal()

```


```{r}
# List of numerical features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# Extract numerical feature data
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Remove duplicate rows
numeric_data <- numeric_data %>%
  distinct()

# Standardize numerical features
numeric_data_scaled <- scale(numeric_data)

# Set random seed to ensure reproducibility
set.seed(123)

# Perform t-SNE analysis
tsne_result <- Rtsne(numeric_data_scaled, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)


```


```{r}
# Extract t-SNE results
tsne_data <- as.data.frame(tsne_result$Y)
colnames(tsne_data) <- c("Dim1", "Dim2")

# Merge t-SNE results with the claim_status from the original data
tsne_data <- cbind(tsne_data, claim_status = car_data2$claim_status[1:nrow(tsne_data)])

# Convert claim_status to a factor variable
tsne_data$claim_status <- as.factor(tsne_data$claim_status)

# Visualize t-SNE results
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = claim_status)) +
  geom_point(alpha = 0.7) +
  labs(title = "t-SNE Analysis of Vehicle Data", x = "Dimension 1", y = "Dimension 2") +
  scale_color_manual(values = c("0" = "#00AFBB", "1" = "#FC4E07")) +
  theme_minimal()


```


```{r}
# Set random seed to ensure reproducibility
set.seed(123)

# Perform K-means clustering on the t-SNE results
kmeans_result <- kmeans(tsne_data[, c("Dim1", "Dim2")], centers = 5) # Assuming 5 clusters

# Add clustering results to the data
tsne_data$cluster <- as.factor(kmeans_result$cluster)

# Visualize K-means clustering results
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "t-SNE and K-means Clustering of Vehicle Data", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()

```


```{r}
# Load necessary libraries
library(cluster)
library(dendextend)
library(ggplot2)

# Set random seed to ensure reproducibility
set.seed(123)

# Perform random sampling on the t-SNE results
sample_indices <- sample(1:nrow(tsne_data), size = 10000)
tsne_sample <- tsne_data[sample_indices, ]

# Perform hierarchical clustering on the t-SNE sample
dist_matrix <- dist(tsne_sample[, c("Dim1", "Dim2")])
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Convert to dendrogram object and remove labels
dend <- as.dendrogram(hc_result)
dend <- dend %>% set("labels", NULL) # Remove labels

# Plot dendrogram
plot(dend, main = "Dendrogram of t-SNE Sample Results")

# Cut the dendrogram at different levels and visualize the results
# Assume we want to observe 3 different levels (2 clusters, 3 clusters, and 5 clusters)
k_values <- c(2, 3, 5)
colors <- c("red", "blue", "green", "orange", "purple")

for (k in k_values) {
  # Cut the dendrogram
  clusters <- cutree(hc_result, k = k)
  tsne_sample$cluster <- as.factor(clusters)
  
  # Visualize clustering results
  p <- ggplot(tsne_sample, aes(x = Dim1, y = Dim2, color = cluster)) +
    geom_point(alpha = 0.7) +
    labs(title = paste("t-SNE and Hierarchical Clustering with", k, "Clusters"), 
         x = "Dimension 1", y = "Dimension 2") +
    scale_color_manual(values = colors[1:k]) +
    theme_minimal()
  
  print(p)
}
```


```{r}
# Load necessary libraries
library(cluster)
library(dendextend)
library(ggplot2)
library(dplyr)

# Set random seed to ensure reproducibility
set.seed(123)

# Perform random sampling on the t-SNE results
sample_indices <- sample(1:nrow(tsne_data), size = 10000)
tsne_sample <- tsne_data[sample_indices, ]

# Perform hierarchical clustering on the t-SNE sample
dist_matrix <- dist(tsne_sample[, c("Dim1", "Dim2")])
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Convert to dendrogram object and remove labels
dend <- as.dendrogram(hc_result)
dend <- dend %>% set("labels", NULL) # Remove labels

# Plot dendrogram
plot(dend, main = "Dendrogram of t-SNE Sample Results")

# Cut the dendrogram at different levels and visualize the results
# Assume we want to observe 3 different levels (2 clusters, 3 clusters, and 5 clusters)
k_values <- c(2, 3, 5)
colors <- c("red", "blue", "green", "orange", "purple")

for (k in k_values) {
  # Cut the dendrogram
  clusters <- cutree(hc_result, k = k)
  tsne_sample$cluster <- as.factor(clusters)
  
  # Calculate the proportion of each cluster
  cluster_proportions <- prop.table(table(tsne_sample$cluster))
  print(paste("Proportions of clusters for k =", k))
  print(cluster_proportions)
  
  # Merge cluster information with the original data
  merged_data <- cbind(tsne_sample, car_data2[sample_indices, numeric_features])
  
  # Summarize characteristics of each cluster
  cluster_summary <- merged_data %>%
    group_by(cluster) %>%
    summarise_all(mean, na.rm = TRUE)
  
  print(paste("Cluster characteristics for k =", k))
  print(cluster_summary)
  
  # Visualize clustering results
  p <- ggplot(tsne_sample, aes(x = Dim1, y = Dim2, color = cluster)) +
    geom_point(alpha = 0.7) +
    labs(title = paste("t-SNE and Hierarchical Clustering with", k, "Clusters"), 
         x = "Dimension 1", y = "Dimension 2") +
    scale_color_manual(values = colors[1:k]) +
    theme_minimal()
  
  print(p)
}

```
