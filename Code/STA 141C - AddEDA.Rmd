---
title: "STA 141C - AddEDA"
author: "Johnson Tian"
date: "2024-06-03"
output: pdf_document
---

```{r setup, include=FALSE}
car_data2 <- read.csv("~/Downloads/car_data2.csv")
library(randomForest)
library(readr)
library(dplyr)
library(dplyr)
library(readr)
library(broom)
library(foreach)
library(boot)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(dplyr)
library(xgboost)
library(caret)
library(dplyr)
library(readr)
library(e1071)
library(caret)
library(dplyr)
library(ggplot2)
library(dplyr)
library(factoextra)
library(readr)
```

```{r}
# Load necessary library
library(dplyr)

# Define the numeric features
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# Extract numerical features
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# Save the numeric data to a new CSV file
write.csv(numeric_data, file = "numeric_features.csv", row.names = FALSE)

# To save the file to the download directory (adjust path as needed)
download_path <- "~/Downloads/numeric_features.csv"
write.csv(numeric_data, file = download_path, row.names = FALSE)


```

```{r}




```



```{r}
# Load necessary libraries
library(randomForest)
library(readr)

# Load the data

# Ensure subscription_length is a numeric variable
numeric_data$subscription_length <- as.numeric(numeric_data$subscription_length)

# Train the Random Forest model
set.seed(141)  # For reproducibility
rf_model <- randomForest(subscription_length ~ ., data = numeric_data, importance = TRUE)

# Print the model summary
print(rf_model)

# Get the importance of each feature
importance <- importance(rf_model)
importance_df <- data.frame(Feature = rownames(importance), Importance = importance[, "IncNodePurity"])

# Sort by importance
importance_df <- importance_df[order(-importance_df$Importance), ]

# Print the feature importance
print(importance_df)

# Save the importance to a CSV file
```

```{r}
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Importance for Subscription Length",
       x = "Features",
       y = "Importance")

ggplot(importance_df, aes(x = Importance, y = reorder(Feature, Importance))) +
  geom_point(size = 3) +
  theme_minimal() +
  labs(title = "Feature Importance for Subscription Length",
       x = "Importance",
       y = "Features")

importance_df$Fraction <- importance_df$Importance / sum(importance_df$Importance)
importance_df <- importance_df[importance_df$Fraction > 0, ]  # Remove zero-importance features if any

ggplot(importance_df, aes(x = "", y = Fraction, fill = Feature)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  theme_minimal() +
  labs(title = "Feature Importance for Subscription Length",
       x = "",
       y = "Importance Fraction") +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank())
```


```{r}
# Load necessary libraries
library(dplyr)
library(readr)

# Define the binary features
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert','claim_status'
)

# Convert binary features from "Yes"/"No" to 1/0
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# Select only the binary features
binary_data <- car_data2 %>% select(all_of(binary_features))

# Save the binary features to a new CSV file
write.csv(binary_data, file = "~/Downloads/binary_features.csv", row.names = FALSE)

# Optional: Save to the working directory for easier access
write.csv(binary_data, file = "binary_features.csv", row.names = FALSE)

```


```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(broom)

# Fit logistic regression model
logistic_model <- glm(claim_status ~ ., data = binary_data, family = binomial)

# Summarize the model
summary(logistic_model)

# Extract feature importance
feature_importance <- tidy(logistic_model)

# Save the feature importance to a CSV file
write.csv(feature_importance, file = "~/Downloads/feature_importance_logistic_regression.csv", row.names = FALSE)

# Print feature importance
print(feature_importance)


```


```{r}
# Define categorized binary features
safety_systems <- c('is_esc', 'is_tpms', 'is_brake_assist', 'is_speed_alert')
parking_assistance <- c('is_parking_sensors', 'is_parking_camera')
vehicle_control_comfort <- c('is_adjustable_steering', 'is_power_steering', 'is_driver_seat_height_adjustable')
locking_and_anti_theft <- c('is_power_door_locks', 'is_central_locking')
lighting_and_visibility <- c('is_front_fog_lights', 'is_rear_window_wiper', 'is_rear_window_washer', 'is_rear_window_defogger', 'is_day_night_rear_view_mirror')
other_features <- c('is_ecw')
target_variable <- 'claim_status'

# Combine all features for convenience
binary_features <- c(
  safety_systems,
  parking_assistance,
  vehicle_control_comfort,
  locking_and_anti_theft,
  lighting_and_visibility,
  other_features,
  target_variable
)


```


```{r}
# 加载数据
binary_features <- read.csv("binary_features.csv")
new_binary_features_binarized <- read.csv("~/new_binary_features_binarized.csv")

# 创建新变量
binary_features$safety_systems <- binary_features$is_esc + binary_features$is_tpms + binary_features$is_brake_assist + binary_features$is_speed_alert
binary_features$parking_assistance <- binary_features$is_parking_sensors + binary_features$is_parking_camera
binary_features$vehicle_control_comfort <- binary_features$is_adjustable_steering + binary_features$is_power_steering + binary_features$is_driver_seat_height_adjustable
binary_features$locking_and_anti_theft <- binary_features$is_power_door_locks + binary_features$is_central_locking
binary_features$lighting_and_visibility <- binary_features$is_front_fog_lights + binary_features$is_rear_window_wiper + binary_features$is_rear_window_washer + binary_features$is_rear_window_defogger + binary_features$is_day_night_rear_view_mirror

# 选择需要的列
new_binary_features <- binary_features[, c('safety_systems', 'parking_assistance', 'vehicle_control_comfort', 'locking_and_anti_theft', 'lighting_and_visibility', 'claim_status')]

# 保存数据至下载文件夹
write.csv(new_binary_features, file = "~/Downloads/new_binary_features.csv", row.names = FALSE)

# 可选: 保存到工作目录以便于访问
write.csv(new_binary_features, file = "new_binary_features.csv", row.names = FALSE)

```


```{r}
# 加载数据
new_binary_features <- read.csv("~/Downloads/new_binary_features.csv")

# 创建函数进行归位操作
binarize <- function(column) {
  threshold <- (max(column) + min(column)) / 2
  return(ifelse(column > threshold, 1, 0))
}

new_binary_features$safety_systems <- binarize(new_binary_features$safety_systems)
new_binary_features$parking_assistance <- binarize(new_binary_features$parking_assistance)
new_binary_features$vehicle_control_comfort <- binarize(new_binary_features$vehicle_control_comfort)
new_binary_features$locking_and_anti_theft <- binarize(new_binary_features$locking_and_anti_theft)
new_binary_features$lighting_and_visibility <- binarize(new_binary_features$lighting_and_visibility)

# 保存更新后的数据至下载文件夹
write.csv(new_binary_features, file = "~/Downloads/new_binary_features_binarized.csv", row.names = FALSE)

# 可选: 保存到工作目录以便于访问
write.csv(new_binary_features, file = "new_binary_features_binarized.csv", row.names = FALSE)


```


```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(broom)

# Load the new binary features data

# Ensure claim_status is treated as a factor for logistic regression
new_binary_features_binarized$claim_status <- as.factor(new_binary_features_binarized$claim_status)

# Fit logistic regression model
logistic_model <- glm(claim_status ~ ., data = new_binary_features_binarized, family = binomial)

# Summarize the model
summary(logistic_model)

# Extract feature importance
feature_importance <- tidy(logistic_model)

# Print feature importance
print(feature_importance)

# Save the feature importance to a CSV file
write.csv(feature_importance, file = "~/Downloads/feature_importance_logistic_regression.csv", row.names = FALSE)


```


```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(broom)
library(ggplot2)

# Load the new binary features data

# Ensure claim_status is treated as a factor for logistic regression
new_binary_features_binarized$claim_status <- as.factor(new_binary_features_binarized$claim_status)

# Fit logistic regression model
logistic_model <- glm(claim_status ~ ., data = new_binary_features_binarized, family = binomial)

# Extract feature importance
feature_importance <- tidy(logistic_model, conf.int = TRUE)

# Plot the coefficients
ggplot(feature_importance, aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Logistic Regression Coefficients with 95% CI",
       x = "Features",
       y = "Coefficient Estimate") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")

summary(logistic_model)


```


```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(ggplot2)

# Load the new binary features data

# Ensure claim_status is treated as a factor for logistic regression
new_binary_features_binarized$claim_status <- as.factor(new_binary_features_binarized$claim_status)

# Fit logistic regression model
logistic_model <- glm(claim_status ~ ., data = new_binary_features_binarized, family = binomial)

# Extract feature importance
feature_importance <- tidy(logistic_model)

# For pie chart, calculate the absolute value of the estimates
feature_importance <- feature_importance %>%
  filter(term != "(Intercept)") %>%
  mutate(abs_estimate = abs(estimate))

# Create pie chart
ggplot(feature_importance, aes(x = "", y = abs_estimate, fill = term)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  theme_minimal() +
  labs(title = "Feature Importance (Absolute Values)",
       fill = "Features") +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank())


```


```{r}
# Plot the coefficients with error bars for confidence intervals
ggplot(feature_importance, aes(x = reorder(term, estimate), y = estimate)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error), width = 0.2) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Logistic Regression Coefficients",
       x = "Features",
       y = "Coefficient Estimate") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")

```


```{r}
# Load necessary library
library(boot)

# Define a function to calculate the mean
mean_func <- function(data, indices) {
  return(mean(data[indices]))
}

# Load your numeric data (replace with your actual data loading method)
numeric_data <- car_data2[numeric_features]

# Select a specific numeric feature for demonstration
selected_feature <- numeric_data$subscription_length

# Perform bootstrap
boot_result <- boot(selected_feature, mean_func, R = 1000)

# Print bootstrap results
print(boot_result)

# Calculate confidence intervals
boot.ci(boot_result, type = "perc")


# Define a function to calculate linear regression coefficients
lm_func <- function(data, indices) {
  d <- data[indices, ]  # Select the resampled data
  fit <- lm(subscription_length ~ ., data = d)
  return(coef(fit))
}

# Perform bootstrap
boot_result <- boot(numeric_data, lm_func, R = 1000)

# Print bootstrap results
print(boot_result)

# Calculate confidence intervals for the coefficients
for (i in 1:length(boot_result$t0)) {
  cat("Coefficient", names(boot_result$t0)[i], "\n")
  print(boot.ci(boot_result, type = "perc", index = i))
}

# Load necessary library
library(randomForest)

# Define a function to calculate variable importance
rf_func <- function(data, indices) {
  d <- data[indices, ]  # Select the resampled data
  fit <- randomForest(subscription_length ~ ., data = d)
  return(importance(fit))
}

# Perform bootstrap
boot_result <- boot(numeric_data, rf_func, R = 100)

# Aggregate and print variable importance
var_importance <- apply(boot_result$t, 2, mean)
print(var_importance)

```


```{r}
# Load necessary libraries
library(randomForest)
library(readr)
library(boot)

# Load your data (replace with your actual data loading method)
# car_data2 <- read_csv("/path/to/your/data.csv")
# For this example, assuming numeric_data is already loaded with appropriate variables
# Ensure subscription_length is a numeric variable
numeric_data$subscription_length <- as.numeric(numeric_data$subscription_length)

# Define a function to train the model and return the importance of features
rf_func <- function(data, indices) {
  d <- data[indices, ]  # Select the resampled data
  fit <- randomForest(subscription_length ~ ., data = d, importance = TRUE)
  return(importance(fit)[, "IncNodePurity"])  # Return importance scores for each feature
}

# Perform bootstrap
set.seed(141)  # For reproducibility
boot_result <- boot(numeric_data, rf_func, R = 5)  # R = 1000 bootstrap samples

# Aggregate the results
importance_means <- apply(boot_result$t, 2, mean)
importance_sds <- apply(boot_result$t, 2, sd)
importance_df <- data.frame(Feature = names(importance_means), 
                            MeanImportance = importance_means, 
                            SDImportance = importance_sds)

# Sort by mean importance
importance_df <- importance_df[order(-importance_df$MeanImportance), ]

# Print the feature importance with bootstrap estimates
print(importance_df)

# Save the results to a CSV file
write.csv(importance_df, file = "~/Downloads/bootstrap_feature_importance.csv", row.names = FALSE)

# Plot the feature importance with error bars
library(ggplot2)

ggplot(importance_df, aes(x = reorder(Feature, MeanImportance), y = MeanImportance)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black", alpha = 0.7) +
  geom_errorbar(aes(ymin = MeanImportance - 1.96 * SDImportance, ymax = MeanImportance + 1.96 * SDImportance), width = 0.2) +
  coord_flip() +
  labs(title = "Bootstrap Aggregated Variable Importance from Random Forest",
       x = "Variables",
       y = "Importance (Mean ± 1.96*SD)") +
  theme_minimal()


```


```{r}
# Detect the number of available cores
num_cores <- detectCores()

# Register the parallel backend
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Define a parallel version of the boot function
parallel_boot <- function(data, statistic, R) {
  results <- foreach(i = 1:R, .combine = rbind, .packages = "randomForest") %dopar% {
    indices <- sample(1:nrow(data), replace = TRUE)
    statistic(data, indices)
  }
  return(list(t = results))
}

# Perform parallel bootstrap
set.seed(141)  # For reproducibility
boot_result <- parallel_boot(numeric_data, rf_func, R = 500)  # Adjust R as needed

# Stop the cluster
stopCluster(cl)



```


```{r}
# Calculate correlation matrix for numeric variables
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Visualize correlation matrix
library(reshape2)

cor_melt <- melt(cor_matrix)
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  labs(title = "Correlation Matrix") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




```


```{r}
# Assuming you have a data frame 'categorical_data' with categorical features
categorical_features <- c('region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power', 'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type')

# Load necessary library for combining data
library(tidyr)

# Combine numeric and categorical data (assuming both data frames have the same rows)
full_data <- cbind(numeric_data, categorical_data)

# Boxplot for each categorical variable against subscription_length
for (feature in categorical_features) {
  p <- ggplot(full_data, aes_string(x = feature, y = "subscription_length")) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Boxplot of Subscription Length by", feature),
         x = feature,
         y = "Subscription Length") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(p)
}


```


```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# 检查两个数据框的行数
nrow(numeric_data)
nrow(categorical_data)

# 假设 categorical_data 存在缺失值，删除含有缺失值的行
categorical_data_clean <- categorical_data %>% drop_na()

# 确认两个数据框的行数匹配
if (nrow(numeric_data) == nrow(categorical_data_clean)) {
  # 合并数据框
  full_data <- cbind(numeric_data, categorical_data_clean)
} else {
  stop("Row numbers do not match after cleaning the categorical data.")
}

# Print the dimensions of the merged data frame
print(dim(full_data))


```


```{r}
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type','subscription_length', 'claim_status'
)
categorical_data <- car_data2 %>%
  select(all_of(categorical_features))
```


```{r}
# Cross-tabulation for each pair of categorical variables
for (i in 1:(length(categorical_features) - 2)) {
  for (j in (i + 1):(length(categorical_features) - 2)) {
    cat("Cross-tabulation between", categorical_features[i], "and", categorical_features[j], "\n")
    print(table(categorical_data[[categorical_features[i]]], categorical_data[[categorical_features[j]]]))
  }
}


```


```{r}
# Perform Chi-Square Test for each pair of categorical variables
chi_square_results <- list()

for (i in 1:(length(categorical_features) - 2)) {
  for (j in (i + 1):(length(categorical_features) - 2)) {
    chi_test <- chisq.test(table(categorical_data[[categorical_features[i]]], categorical_data[[categorical_features[j]]]))
    chi_square_results[[paste(categorical_features[i], "vs", categorical_features[j])]] <- chi_test
    cat("Chi-Square Test between", categorical_features[i], "and", categorical_features[j], "\n")
    print(chi_test)
  }
}


```


```{r}
# Boxplot for subscription_length by each categorical variable
for (feature in setdiff(categorical_features, c('subscription_length', 'claim_status'))) {
  p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length")) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Boxplot of Subscription Length by", feature),
         x = feature,
         y = "Subscription Length") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(p)
}

# Bar plot for claim_status by each categorical variable
for (feature in setdiff(categorical_features, c('subscription_length', 'claim_status'))) {
  p <- ggplot(categorical_data, aes_string(x = feature, fill = "claim_status")) +
    geom_bar(position = "fill") +
    theme_minimal() +
    labs(title = paste("Proportion of Claim Status by", feature),
         x = feature,
         y = "Proportion") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(p)
}


```


```{r}
# Cross-tabulation and stacked bar chart for each pair of categorical variables
for (i in 1:(length(categorical_features) - 2)) {
  for (j in (i + 1):(length(categorical_features) - 2)) {
    p <- ggplot(categorical_data, aes_string(x = categorical_features[i], fill = categorical_features[j])) +
      geom_bar(position = "fill") +
      theme_minimal() +
      labs(title = paste("Proportion of", categorical_features[j], "by", categorical_features[i]),
           x = categorical_features[i],
           y = "Proportion") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(p)
  }
}



```


```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# 提取需要的分类变量和目标变量
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# 生成箱线图
for (feature in categorical_features) {
  p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length")) +
    geom_boxplot(fill = "skyblue", color = "black", alpha = 0.8) +
    theme_minimal() +
    labs(title = paste("Boxplot of Subscription Length by", feature),
         x = feature,
         y = "Subscription Length") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(p)
}


```


```{r}
# Load necessary libraries
library(dplyr)
library(e1071)  # for naiveBayes
library(caret)  # for data partitioning

# 提取需要的分类变量和目标变量
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# 确保 subscription_length 是数值变量
car_data2$subscription_length <- as.numeric(car_data2$subscription_length)

# 将 subscription_length 离散化为分类变量（例如，分为 "短", "中", "长" 三类）
car_data2$subscription_length_cat <- cut(car_data2$subscription_length, breaks = quantile(car_data2$subscription_length, probs = seq(0, 1, by = 0.33)), labels = c("short", "medium", "long"), include.lowest = TRUE)

# 提取分类变量和目标变量
categorical_data <- car_data2 %>%
  select(all_of(categorical_features), subscription_length_cat, claim_status)

# 将分类变量转换为因子
categorical_data <- categorical_data %>%
  mutate(across(everything(), as.factor))

# 检查数据
str(categorical_data)


```


```{r}
# 创建用于训练和测试的数据划分
set.seed(123)
trainIndex <- createDataPartition(categorical_data$subscription_length_cat, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- categorical_data[trainIndex, ]
test_data  <- categorical_data[-trainIndex, ]

# 训练朴素贝叶斯模型
nb_model_length <- naiveBayes(subscription_length_cat ~ ., data = train_data)

# 使用测试数据进行预测
predictions_length <- predict(nb_model_length, newdata = test_data)

# 评估模型性能
conf_matrix_length <- confusionMatrix(predictions_length, test_data$subscription_length_cat)
print(conf_matrix_length)


```


```{r}
# 创建用于训练和测试的数据划分
set.seed(123)
trainIndex <- createDataPartition(categorical_data$claim_status, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- categorical_data[trainIndex, ]
test_data  <- categorical_data[-trainIndex, ]

# 训练朴素贝叶斯模型
nb_model_claim <- naiveBayes(claim_status ~ ., data = train_data)

# 使用测试数据进行预测
predictions_claim <- predict(nb_model_claim, newdata = test_data)

# 评估模型性能
conf_matrix_claim <- confusionMatrix(predictions_claim, test_data$claim_status)
print(conf_matrix_claim)


```

```{r}
# Load necessary libraries
library(UBL)
library(randomForest)
library(caret)

# 对 claim_status 数据进行 SMOTE 处理
balanced_data <- SmoteClassif(claim_status ~ ., data = categorical_data, C.perc = "balance")

# 创建用于训练和测试的数据划分
set.seed(123)
trainIndex <- createDataPartition(balanced_data$claim_status, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- balanced_data[trainIndex, ]
test_data  <- balanced_data[-trainIndex, ]

# 训练随机森林模型
rf_model_claim <- randomForest(claim_status ~ ., data = train_data, importance = TRUE, ntree = 100)

# 使用测试数据进行预测
predictions_claim <- predict(rf_model_claim, newdata = test_data)

# 评估模型性能
conf_matrix_claim <- confusionMatrix(predictions_claim, test_data$claim_status)
print(conf_matrix_claim)



```


```{r}
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type','subscription_length', 'vehicle_age'
)
categorical_data <- car_data2 %>%
  select(all_of(categorical_features))

```


```{r}
library(dplyr)

# 对每个分类变量进行卡方检验
chi_square_results <- lapply(categorical_features[-12], function(feature) {
  contingency_table <- table(categorical_data[[feature]], categorical_data$claim_status)
  chi_square_test <- chisq.test(contingency_table)
  return(list(feature = feature, p_value = chi_square_test$p.value))
})

# 打印卡方检验结果
chi_square_results <- do.call(rbind, chi_square_results)
print(chi_square_results)





```

```{r}
library(caret)

# 线性回归模型预测 subscription_length
linear_model <- lm(subscription_length ~ ., data = categorical_data)
summary(linear_model)

# 逻辑回归模型预测 claim_status
logistic_model <- glm(claim_status ~ ., data = categorical_data, family = binomial)
summary(logistic_model)


```


```{r}
for (feature in categorical_features[-11]) {  # 排除 subscription_length 本身
  p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length")) +
    geom_boxplot() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    xlab(feature) +
    ylab("Subscription Length") +
    ggtitle(paste("Boxplot of Subscription Length by", feature)) +
    theme_minimal()
  print(p)
  ggsave(paste0("boxplot_subscription_length_by_", feature, ".png"), plot = p)
}



```


```{r}
library(ggplot2)
library(dplyr)

# 对 subscription_length 进行小提琴图分析
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length", fill = feature)) +
      geom_violin() +
      geom_boxplot(width = 0.1, fill = "white") +
      labs(title = paste("Subscription Length vs", feature), x = feature, y = "Subscription Length") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(p)
  }
}



```


```{r}
library(plotly)
library(ggplot2)
library(dplyr)
library(plotly)
library(ggplot2)
library(dplyr)



# 对 subscription_length 和 vehicle_age 进行三维分析
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length", fill = feature)) +
      geom_violin() +
      geom_boxplot(width = 0.1, fill = "white") +
      labs(title = paste("Subscription Length vs", feature), x = feature, y = "Subscription Length") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # 使用 plotly 转换为交互式图表
    ggplotly(p)
  }
}


# 创建三维散点图
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    p <- plot_ly(categorical_data, 
                 x = ~get(feature), 
                 y = ~subscription_length, 
                 z = ~vehicle_age, 
                 type = "scatter3d", 
                 mode = "markers",
                 marker = list(size = 3, color = ~as.factor(get(feature)), colorscale = "Viridis"),
                 text = ~paste("Subscription Length:", subscription_length, "<br>Vehicle Age:", vehicle_age)) %>%
      layout(title = paste("3D Scatter Plot of", feature, "vs Subscription Length and Vehicle Age"),
             scene = list(xaxis = list(title = feature),
                          yaxis = list(title = "Subscription Length"),
                          zaxis = list(title = "Vehicle Age")))
    print(p)
  }
}

```

```{r}
# 自定义颜色向量
colors <- c("red", "blue", "green", "purple", "orange", "brown", "pink", "gray", "yellow", "cyan")

# 对 subscription_length 和 vehicle_age 进行三维分析
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    unique_levels <- unique(categorical_data[[feature]])
    color_mapping <- setNames(colors[1:length(unique_levels)], unique_levels)
    
    p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length", fill = feature)) +
      geom_violin() +
      geom_boxplot(width = 0.1, fill = "white") +
      scale_fill_manual(values = color_mapping) +
      labs(title = paste("Subscription Length vs", feature), x = feature, y = "Subscription Length") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # 使用 plotly 转换为交互式图表
    ggplotly(p) %>%
      layout(title = paste("Subscription Length vs", feature),
             xaxis = list(title = feature),
             yaxis = list(title = "Subscription Length"))
  }
}

# 创建三维散点图
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    unique_levels <- unique(categorical_data[[feature]])
    color_mapping <- setNames(colors[1:length(unique_levels)], unique_levels)
    
    p <- plot_ly(categorical_data, 
                 x = ~get(feature), 
                 y = ~subscription_length, 
                 z = ~vehicle_age, 
                 type = "scatter3d", 
                 mode = "markers",
                 marker = list(size = 3, color = ~as.factor(get(feature)), colors = color_mapping),
                 text = ~paste("Subscription Length:", subscription_length, "<br>Vehicle Age:", vehicle_age)) %>%
      layout(title = paste("3D Scatter Plot of", feature, "vs Subscription Length and Vehicle Age"),
             scene = list(xaxis = list(title = feature),
                          yaxis = list(title = "Subscription Length"),
                          zaxis = list(title = "Vehicle Age")))
    print(p)
  }
}


```

```{r}
library(ggplot2)
library(dplyr)

# 对 subscription_length 进行小提琴图分析
for (feature in categorical_features) {
  if (feature != "subscription_length" && feature != "claim_status") {
    p <- ggplot(categorical_data, aes_string(x = feature, y = "subscription_length", fill = feature)) +
      geom_violin() +
      geom_boxplot(width = 0.1, fill = "white") +
      labs(title = paste("Subscription Length vs", feature), x = feature, y = "Subscription Length") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(p)
  }
}



```


```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)

# 确保 categorical_data 包含所有必要的分类特征和目标变量
categorical_features <- c(
  'region_code', 'segment', 'model', 'fuel_type', 'max_torque', 'max_power',
  'engine_type', 'rear_brakes_type', 'transmission_type', 'steering_type'
)

# 选择所有特征和目标变量
categorical_data <- car_data2 %>%
  select(all_of(categorical_features), claim_status)

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(categorical_data$claim_status, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- categorical_data[trainIndex, ]
test_data <- categorical_data[-trainIndex, ]

# 确保训练集和测试集中的因子水平相同
train_data$claim_status <- factor(train_data$claim_status)
test_data$claim_status <- factor(test_data$claim_status, levels = levels(train_data$claim_status))

# 构建决策树模型
decision_tree <- rpart(claim_status ~ ., data = train_data, method = "class")

# 打印决策树模型
print(decision_tree)

# 可视化决策树
rpart.plot(decision_tree, extra = 106, under = TRUE, 
           faclen = 0, cex = 0.8, main = "Decision Tree for Predicting Claim Status")

# 预测测试集
predictions <- predict(decision_tree, test_data, type = "class")

# 将预测值和实际值转换为因子，并确保它们具有相同的水平
predictions <- factor(predictions, levels = levels(test_data$claim_status))

# 评估模型性能
confusionMatrix(predictions, test_data$claim_status)



```


```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)

# 确保 car_data2 包含所有必要的数值特征和目标变量
new_numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating','claim_status'
)

# 选择所有特征和目标变量
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(numeric_data$subscription_length, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- numeric_data[trainIndex, ]
test_data <- numeric_data[-trainIndex, ]

# 确保训练集和测试集中的因子水平相同
train_data$subscription_length <- as.numeric(train_data$subscription_length)
test_data$subscription_length <- as.numeric(test_data$subscription_length)

# 构建决策树模型
decision_tree <- rpart(subscription_length ~ ., data = train_data, method = "anova")

# 打印决策树模型
print(decision_tree)

# 可视化决策树
rpart.plot(decision_tree, extra = 101, under = TRUE, 
           faclen = 0, cex = 0.8, main = "Decision Tree for Predicting Subscription Length")

# 预测测试集
predictions <- predict(decision_tree, test_data)

# 评估模型性能
mse <- mean((predictions - test_data$subscription_length)^2)
cat("Mean Squared Error:", mse, "\n")



```

```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)

# 确保 car_data2 包含所有必要的数值特征和目标变量
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 选择所有特征和目标变量
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(numeric_data$subscription_length, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- numeric_data[trainIndex, ]
test_data <- numeric_data[-trainIndex, ]

# 确保训练集和测试集中的因子水平相同
train_data$subscription_length <- as.numeric(train_data$subscription_length)
test_data$subscription_length <- as.numeric(test_data$subscription_length)

# 构建决策树模型
decision_tree <- rpart(subscription_length ~ ., data = train_data, method = "anova")

# 打印决策树模型
print(decision_tree)

# 可视化决策树
rpart.plot(decision_tree, extra = 101, under = TRUE, 
           faclen = 0, cex = 0.8, main = "Decision Tree for Predicting Subscription Length")

# 预测测试集
predictions <- predict(decision_tree, test_data)

# 评估模型性能
mse <- mean((predictions - test_data$subscription_length)^2)
cat("Mean Squared Error:", mse, "\n")


```


```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)

# 确保 car_data2 包含所有必要的数值特征和目标变量
new_numeric_features <- c(
  'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating', 'subscription_length'
)

# 选择所有特征和目标变量
numeric_data <- car_data2 %>%
  select(all_of(new_numeric_features))

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(numeric_data$subscription_length, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- numeric_data[trainIndex, ]
test_data <- numeric_data[-trainIndex, ]

# 构建决策树模型
decision_tree <- rpart(subscription_length ~ ., data = train_data, method = "anova")

# 打印决策树模型
print(decision_tree)

# 可视化决策树
rpart.plot(decision_tree, extra = 101, under = TRUE, 
           faclen = 0, cex = 0.8, main = "Decision Tree for Predicting Subscription Length")

# 预测测试集
predictions <- predict(decision_tree, test_data)

# 评估模型性能
mse <- mean((predictions - test_data$subscription_length)^2)
cat("Mean Squared Error:", mse, "\n")



```


```{r}
library(xgboost)
library(caret)
library(dplyr)

# 确保 car_data2 包含所有必要的数值特征和目标变量
new_numeric_features <- c(
  'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating', 'subscription_length'
)

# 选择所有特征和目标变量
numeric_data <- car_data2 %>%
  select(all_of(new_numeric_features))

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(numeric_data$subscription_length, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- numeric_data[trainIndex, ]
test_data <- numeric_data[-trainIndex, ]

# 将数据转换为矩阵格式以适应 xgboost
train_matrix <- as.matrix(train_data %>% select(-subscription_length))
train_label <- train_data$subscription_length
test_matrix <- as.matrix(test_data %>% select(-subscription_length))
test_label <- test_data$subscription_length

# 构建 xgboost 模型
xgb_train <- xgb.DMatrix(data = train_matrix, label = train_label)
xgb_test <- xgb.DMatrix(data = test_matrix, label = test_label)

params <- list(
  objective = "reg:squarederror",  # 目标函数为回归平方误差
  eval_metric = "rmse",            # 评价指标为均方根误差
  booster = "gbtree",              # 基础分类器为决策树
  eta = 0.1,                       # 学习率
  max_depth = 6                    # 最大树深
)

xgb_model <- xgb.train(
  params = params,
  data = xgb_train,
  nrounds = 100,                   # 迭代轮数
  watchlist = list(train = xgb_train, eval = xgb_test),
  early_stopping_rounds = 10,      # 提前停止轮数
  verbose = 1                      # 打印训练进度
)

# 预测测试集
predictions <- predict(xgb_model, newdata = xgb_test)

# 评估模型性能
mse <- mean((predictions - test_label)^2)
cat("Mean Squared Error:", mse, "\n")

# 可视化特征重要性并添加标题
importance <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
importance_plot <- xgb.plot.importance(importance_matrix = importance, main = "Feature Importance Predicting Sub Length using GBTree")


```

```{r}
# 使用 caret 包进行超参数调优
grid <- expand.grid(
  nrounds = c(50, 100, 150),
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.1, 0.3),
  gamma = c(0, 1, 5),
  colsample_bytree = c(0.5, 0.7, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.7, 1)
)

train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

xgb_train <- train(
  x = train_matrix, 
  y = train_label,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = grid
)

# 打印最佳参数
print(xgb_train$bestTune)

# 使用最佳参数进行模型训练
best_params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  booster = "gbtree",
  eta = xgb_train$bestTune$eta,
  max_depth = xgb_train$bestTune$max_depth,
  gamma = xgb_train$bestTune$gamma,
  colsample_bytree = xgb_train$bestTune$colsample_bytree,
  min_child_weight = xgb_train$bestTune$min_child_weight,
  subsample = xgb_train$bestTune$subsample
)

xgb_model <- xgb.train(
  params = best_params,
  data = xgb_train,
  nrounds = xgb_train$bestTune$nrounds,
  watchlist = list(train = xgb_train, eval = xgb_test),
  early_stopping_rounds = 10,
  verbose = 1
)


```


```{r}
# 定义二元特征
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert', 'claim_status'
)

# 将二元特征从 "Yes"/"No" 转换为 1/0
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# 选择仅包含二元特征的数据
binary_data <- car_data2 %>% select(all_of(binary_features))

# 确保所有特征都是数值型
binary_data[] <- lapply(binary_data, as.numeric)

# 将 claim_status 转换为因子
binary_data$claim_status <- as.factor(binary_data$claim_status)

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(binary_data$claim_status, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- binary_data[trainIndex, ]
test_data <- binary_data[-trainIndex, ]

# 将数据转换为矩阵格式以适应 xgboost
train_matrix <- as.matrix(train_data %>% select(-claim_status))
train_label <- as.numeric(train_data$claim_status) - 1  # 将因子转换为数值
test_matrix <- as.matrix(test_data %>% select(-claim_status))
test_label <- as.numeric(test_data$claim_status) - 1

# 构建 xgboost 模型
xgb_train <- xgb.DMatrix(data = train_matrix, label = train_label)
xgb_test <- xgb.DMatrix(data = test_matrix, label = test_label)

params <- list(
  objective = "binary:logistic",   # 目标函数为二元逻辑回归
  eval_metric = "logloss",         # 评价指标为对数损失
  booster = "gbtree",              # 基础分类器为决策树
  eta = 0.1,                       # 学习率
  max_depth = 6                    # 最大树深
)

xgb_model <- xgb.train(
  params = params,
  data = xgb_train,
  nrounds = 100,                   # 迭代轮数
  watchlist = list(train = xgb_train, eval = xgb_test),
  early_stopping_rounds = 10,      # 提前停止轮数
  verbose = 1                      # 打印训练进度
)

# 预测测试集
predictions <- predict(xgb_model, newdata = xgb_test)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# 评估模型性能
confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test_label))
print(confusion_matrix)

```


```{r}
library(e1071)
library(caret)
library(dplyr)
library(readr)
library(ggplot2)
library(e1071)
library(caret)
library(dplyr)
library(readr)
library(ggplot2)
library(ROSE)
# 定义二元特征
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert', 'claim_status'
)

# 将二元特征从 "Yes"/"No" 转换为 1/0
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# 选择仅包含二元特征的数据
binary_data <- car_data2 %>% select(all_of(binary_features))

# 确保所有特征都是数值型
binary_data[] <- lapply(binary_data, as.numeric)

# 将 claim_status 转换为因子
binary_data$claim_status <- as.factor(binary_data$claim_status)

# 将数据分为训练集和测试集
set.seed(123)
trainIndex <- createDataPartition(binary_data$claim_status, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- binary_data[trainIndex, ]
test_data <- binary_data[-trainIndex, ]

# 使用 ROSE 包进行上采样
train_data_balanced <- ROSE(claim_status ~ ., data = train_data, seed = 123)$data

# 构建 SVM 模型
svm_model <- svm(claim_status ~ ., data = train_data_balanced, kernel = "linear", probability = TRUE)

# 打印模型摘要
print(svm_model)

# 预测测试集
predictions <- predict(svm_model, newdata = test_data, probability = TRUE)

# 将预测结果转换为因子
predicted_classes <- factor(predictions, levels = levels(test_data$claim_status))

# 评估模型性能
confusion_matrix <- confusionMatrix(predicted_classes, test_data$claim_status)
print(confusion_matrix)

# 为可视化选择两个特征
visualization_features <- c('is_esc', 'is_adjustable_steering')

# 创建训练集和测试集的数据框，仅包含所选特征
train_data_vis <- train_data_balanced %>% select(all_of(visualization_features), claim_status)
test_data_vis <- test_data %>% select(all_of(visualization_features), claim_status)

# 重新训练 SVM 模型，仅使用所选特征
svm_model_vis <- svm(claim_status ~ ., data = train_data_vis, kernel = "linear", probability = TRUE)

# 预测测试集，仅使用所选特征
predictions_vis <- predict(svm_model_vis, newdata = test_data_vis, probability = TRUE)

# 可视化分类决策边界
ggplot(data = test_data_vis, aes(x = is_esc, y = is_adjustable_steering, color = claim_status)) +
  geom_point(size = 2) +
  stat_contour(aes(z = as.numeric(predictions_vis)), bins = 1, color = "black", linetype = "dashed") +
  labs(title = "SVM Classification Decision Boundary",
       x = "is_esc",
       y = "is_adjustable_steering")
# 计算特征重要性（示例：基于特征消融）
feature_importance <- data.frame(Feature = character(), Importance = numeric())

for (feature in colnames(train_data_balanced)[-ncol(train_data_balanced)]) {
  # 去掉一个特征重新训练模型
  svm_model_temp <- svm(claim_status ~ ., data = train_data_balanced %>% select(-all_of(feature)), kernel = "linear", probability = TRUE)
  
  # 预测测试集
  predictions_temp <- predict(svm_model_temp, newdata = test_data %>% select(-all_of(feature)), probability = TRUE)
  predicted_classes_temp <- factor(predictions_temp, levels = levels(test_data$claim_status))
  
  # 评估模型性能
  confusion_matrix_temp <- confusionMatrix(predicted_classes_temp, test_data$claim_status)
  accuracy <- confusion_matrix_temp$overall['Accuracy']
  
  # 计算重要性（基于模型准确性的下降）
  importance <- confusion_matrix$overall['Accuracy'] - accuracy
  feature_importance <- rbind(feature_importance, data.frame(Feature = feature, Importance = importance))
}

# 排序特征重要性
feature_importance <- feature_importance %>% arrange(desc(Importance))

# 可视化特征重要性
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance Based on Ablation",
       x = "Feature",
       y = "Importance")

```

```{r}
# 定义数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 提取数值特征数据
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 确保所有特征都是数值型
numeric_data[] <- lapply(numeric_data, as.numeric)

# 去除目标变量 subscription_length 以进行PCA
pca_data <- numeric_data %>% select(-subscription_length)

# 进行PCA分析
pca_result <- prcomp(pca_data, scale. = TRUE)

# 打印PCA结果
summary(pca_result)

# 可视化主成分贡献率
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

# 可视化变量对主成分的贡献
fviz_pca_var(pca_result, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# 可视化样本在主成分空间的分布
fviz_pca_ind(pca_result, col.ind = numeric_data$subscription_length, palette = "jco", addEllipses = TRUE, ellipse.level = 0.95, legend.title = "Subscription Length")

```


```{r}
# 定义数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 提取数值特征数据
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 确保所有特征都是数值型
numeric_data[] <- lapply(numeric_data, as.numeric)

# 去除目标变量 subscription_length 以进行PCA
pca_data <- numeric_data %>% select(-subscription_length)

# 进行PCA分析
pca_result <- prcomp(pca_data, scale. = TRUE)

# 打印PCA结果
summary(pca_result)

# 打印每个主成分的重要性
importance <- summary(pca_result)$importance
print(importance)

# 可视化主成分贡献率
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 60))

# 可视化变量对主成分的贡献
fviz_pca_var(pca_result, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# 可视化样本在主成分空间的分布
fviz_pca_ind(pca_result, col.ind = numeric_data$subscription_length, palette = "jco", addEllipses = TRUE, ellipse.level = 0.95, legend.title = "Subscription Length")

```


```{r}
library(glmnet)
library(dplyr)
# 定义数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 提取数值特征数据
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 确保所有特征都是数值型
numeric_data[] <- lapply(numeric_data, as.numeric)

# 去除目标变量 subscription_length 以进行PCA
pca_data <- numeric_data %>% select(-subscription_length)

# 进行PCA分析
pca_result <- prcomp(pca_data, scale. = TRUE)

# 提取前四个主成分
pca_scores <- as.data.frame(pca_result$x[, 1:4])
pca_scores$subscription_length <- numeric_data$subscription_length


# 准备数据
x <- as.matrix(pca_scores %>% select(-subscription_length))
y <- pca_scores$subscription_length

# 岭回归
ridge_model <- glmnet(x, y, alpha = 0)
cv_ridge <- cv.glmnet(x, y, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x)

# 打印最优lambda值
print(best_lambda_ridge)

# 打印岭回归模型系数
ridge_coefficients <- predict(ridge_model, type = "coefficients", s = best_lambda_ridge)
print(ridge_coefficients)

# 计算预测误差
mse <- mean((ridge_predictions - y)^2)
cat("Mean Squared Error:", mse, "\n")

# 绘制预测值与实际值的对比图
plot(y, ridge_predictions, xlab = "Actual Subscription Length", ylab = "Predicted Subscription Length", main = "Ridge Regression: Actual vs Predicted")
abline(0, 1, col = "red")

```

```{r}
# 定义数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 提取数值特征数据
numeric_data <- car_data2 %>%
  select(all_of(numeric_features))

# 确保所有特征都是数值型
numeric_data[] <- lapply(numeric_data, as.numeric)

# 去除目标变量 subscription_length 以进行PCA
pca_data <- numeric_data %>% select(-subscription_length)

# 进行PCA分析
pca_result <- prcomp(pca_data, scale. = TRUE)

# 提取前四个主成分
pca_scores <- as.data.frame(pca_result$x[, 1:4])
pca_scores$subscription_length <- numeric_data$subscription_length

# 准备数据
x <- as.matrix(pca_scores %>% select(-subscription_length))
y <- pca_scores$subscription_length

# Lasso回归
lasso_model <- glmnet(x, y, alpha = 1)
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x)

# 打印最优lambda值
print(best_lambda_lasso)

# 打印Lasso回归模型系数
lasso_coefficients <- predict(lasso_model, type = "coefficients", s = best_lambda_lasso)
print(lasso_coefficients)

# 计算预测误差
mse_lasso <- mean((lasso_predictions - y)^2)
cat("Mean Squared Error (Lasso):", mse_lasso, "\n")

# 绘制预测值与实际值的对比图
plot(y, lasso_predictions, xlab = "Actual Subscription Length", ylab = "Predicted Subscription Length", main = "Lasso Regression: Actual vs Predicted")
abline(0, 1, col = "red")

```


```{r}
# 加载必要的包
library(dplyr)

# 提取数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 对数值特征进行标准化并保存到新的数据框 stan_numeric_data
stan_numeric_data <- car_data2 %>%
  select(all_of(numeric_features)) %>%
  mutate(across(everything(), scale))

# 查看标准化后的数据
head(stan_numeric_data)


```


```{r}
# 加载必要的包
library(dplyr)
library(glmnet)
library(ggplot2)

# 提取数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 对数值特征进行标准化并保存到新的数据框 stan_numeric_data
stan_numeric_data <- car_data2 %>%
  select(all_of(numeric_features)) %>%
  mutate(across(everything(), scale))

# 分离特征和目标变量
x <- as.matrix(stan_numeric_data %>% select(-subscription_length))
y <- stan_numeric_data$subscription_length

# 进行Lasso回归
lasso_model <- cv.glmnet(x, y, alpha = 1)

# 查看最佳lambda值
best_lambda <- lasso_model$lambda.min
print(paste("Best lambda: ", best_lambda))

# 获取模型系数
lasso_coefficients <- coef(lasso_model, s = best_lambda)
print(lasso_coefficients)

# 绘制Lasso路径图
plot(lasso_model)

# 绘制交叉验证曲线
cv_plot <- data.frame(
  log_lambda = log(lasso_model$lambda),
  cvm = lasso_model$cvm,
  cvsd = lasso_model$cvsd
)

ggplot(cv_plot, aes(x = log_lambda, y = cvm)) +
  geom_errorbar(aes(ymin = cvm - cvsd, ymax = cvm + cvsd), width = 0.1) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(best_lambda), linetype = "dashed", color = "red") +
  labs(title = "Lasso Regression Cross-Validation", x = "Log(Lambda)", y = "Mean Squared Error")

# 预测（如果需要）
predictions <- predict(lasso_model, s = best_lambda, newx = x)

# 将预测结果与实际值进行对比
comparison <- data.frame(Actual = y, Predicted = predictions)
head(comparison)


```

```{r}
# 加载必要的包
library(dplyr)
library(glmnet)

# 提取数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 对数值特征进行标准化并保存到新的数据框 stan_numeric_data
stan_numeric_data <- car_data2 %>%
  select(all_of(numeric_features)) %>%
  mutate(across(everything(), scale))

# 分离特征和目标变量
x <- as.matrix(stan_numeric_data %>% select(-subscription_length))
y <- stan_numeric_data$subscription_length

# 进行 Ridge 回归
ridge_model <- cv.glmnet(x, y, alpha = 0)  # alpha = 0 表示 Ridge 回归

# 查看最佳 lambda 值
best_lambda_ridge <- ridge_model$lambda.min
print(paste("Best lambda for Ridge: ", best_lambda_ridge))

# 获取模型系数
ridge_coefficients <- coef(ridge_model, s = best_lambda_ridge)
print(ridge_coefficients)

# 绘制 Ridge 路径图
plot(ridge_model)

# 绘制交叉验证曲线
cv_plot_ridge <- data.frame(
  log_lambda = log(ridge_model$lambda),
  cvm = ridge_model$cvm,
  cvsd = ridge_model$cvsd
)

library(ggplot2)
ggplot(cv_plot_ridge, aes(x = log_lambda, y = cvm)) +
  geom_errorbar(aes(ymin = cvm - cvsd, ymax = cvm + cvsd), width = 0.1) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = log(best_lambda_ridge), linetype = "dashed", color = "red") +
  labs(title = "Ridge Regression Cross-Validation", x = "Log(Lambda)", y = "Mean Squared Error")

# 预测（如果需要）
predictions_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = x)

# 将预测结果与实际值进行对比
comparison_ridge <- data.frame(Actual = y, Predicted = predictions_ridge)
head(comparison_ridge)

```


```{r}
# 加载必要的包
library(dplyr)
library(glmnet)

# 提取数值特征
numeric_features <- c(
  'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
  'airbags', 'displacement', 'cylinder', 'turning_radius', 'length', 'width',
  'gross_weight', 'ncap_rating'
)

# 对数值特征进行标准化并保存到新的数据框 stan_numeric_data
stan_numeric_data <- car_data2 %>%
  select(all_of(numeric_features)) %>%
  mutate(across(everything(), scale))

# 分离特征和目标变量
x <- as.matrix(stan_numeric_data %>% select(-subscription_length))
y <- stan_numeric_data$subscription_length

# 进行 Ridge 回归
ridge_model <- cv.glmnet(x, y, alpha = 0)  # alpha = 0 表示 Ridge 回归

# 查看最佳 lambda 值
best_lambda_ridge <- ridge_model$lambda.min
print(paste("Best lambda for Ridge: ", best_lambda_ridge))

# 获取最佳 lambda 值下的模型系数
ridge_coefficients <- coef(ridge_model, s = best_lambda_ridge)
print(ridge_coefficients)

# 将系数转换为数据框并计算绝对值
coef_df <- as.data.frame(as.matrix(ridge_coefficients))
coef_df$variable <- rownames(coef_df)
names(coef_df) <- c("coefficient", "variable")
coef_df <- coef_df %>% mutate(abs_coefficient = abs(coefficient)) %>% arrange(desc(abs_coefficient))

# 查看按影响大小排序的变量
print(coef_df)

library(ggplot2)

# 绘制变量重要性的条形图
ggplot(coef_df, aes(x = reorder(variable, abs_coefficient), y = abs_coefficient)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance in Predicting Subscription Length using Ridge Regression",
       x = "Variables",
       y = "Absolute Value of Coefficient") +
  theme_minimal()

```


```{r}
# 提取二元特征
binary_features <- c(
  'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
  'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper', 
  'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist', 
  'is_power_door_locks', 'is_central_locking', 'is_power_steering', 
  'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 
  'is_ecw', 'is_speed_alert', 'claim_status'
)

# 将二元特征从 "Yes"/"No" 转换为 1/0
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# 选择二元特征数据
binary_data <- car_data2 %>% select(all_of(binary_features))

# 设置随机种子以保证结果可重复
set.seed(123)

# 拆分数据集为训练集和测试集
train_index <- sample(1:nrow(binary_data), 0.7 * nrow(binary_data))
train_data <- binary_data[train_index, ]
test_data <- binary_data[-train_index, ]

# 训练朴素贝叶斯模型
nb_model <- naiveBayes(claim_status ~ ., data = train_data)
# 使用测试集进行预测
predictions <- predict(nb_model, newdata = test_data)

# 评估模型性能
confusion_matrix <- table(test_data$claim_status, predictions)
print(confusion_matrix)

# 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy: ", accuracy))

```

```{r}
# 安装并加载必要的包
library(DMwR2)

# 欠采样
minority_class <- train_data %>% filter(claim_status == 1)
majority_class <- train_data %>% filter(claim_status == 0)

# 随机抽取多数类样本，使其数量与少数类相同
set.seed(123)
majority_class_under <- majority_class %>% sample_n(nrow(minority_class))

# 合并欠采样后的数据
train_data_under <- rbind(minority_class, majority_class_under)

# 训练朴素贝叶斯模型
nb_model_under <- naiveBayes(claim_status ~ ., data = train_data_under)

# 使用测试集进行预测
predictions_under <- predict(nb_model_under, newdata = test_data)

# 评估模型性能
confusion_matrix_under <- table(test_data$claim_status, predictions_under)
print(confusion_matrix_under)

# 计算准确率
accuracy_under <- sum(diag(confusion_matrix_under)) / sum(confusion_matrix_under)
print(paste("Accuracy (Under-sampling): ", accuracy_under))


```


```{r}
# 使用SMOTE进行过采样
train_data_smote <- SMOTE(claim_status ~ ., train_data, perc.over = 100, perc.under = 200)

# 训练朴素贝叶斯模型
nb_model_smote <- naiveBayes(claim_status ~ ., data = train_data_smote)

# 使用测试集进行预测
predictions_smote <- predict(nb_model_smote, newdata = test_data)

# 评估模型性能
confusion_matrix_smote <- table(test_data$claim_status, predictions_smote)
print(confusion_matrix_smote)

# 计算准确率
accuracy_smote <- sum(diag(confusion_matrix_smote)) / sum(confusion_matrix_smote)
print(paste("Accuracy (SMOTE): ", accuracy_smote))


```


```{r}
train_data_smote <- SMOTE(claim_status ~ ., train_data, perc.over = 100, perc.under = 200)

# 训练朴素贝叶斯模型
nb_model_smote <- naiveBayes(claim_status ~ ., data = train_data_smote)

# 使用测试集进行预测
predictions_smote <- predict(nb_model_smote, newdata = test_data)

# 评估模型性能
confusion_matrix_smote <- table(test_data$claim_status, predictions_smote)
print(confusion_matrix_smote)

# 计算准确率
accuracy_smote <- sum(diag(confusion_matrix_smote)) / sum(confusion_matrix_smote)
print(paste("Accuracy (SMOTE): ", accuracy_smote))

# 计算精确率（Precision）、召回率（Recall）和F1-score
library(caret)
confusion_matrix <- confusionMatrix(predictions_smote, test_data$claim_status)
print(confusion_matrix$byClass["Precision"])
print(confusion_matrix$byClass["Recall"])
print(confusion_matrix$byClass["F1"])

```

```{r}
# 确保所有二元特征都是数值类型
train_data <- train_data %>%
  mutate(across(all_of(binary_features), as.numeric))

# 使用 smotefamily 包中的 SMOTE 方法进行过采样
smote_data <- SMOTE(train_data[, -ncol(train_data)], train_data$claim_status, K = 5, dup_size = 0)

# smote_data$X 是过采样后的特征数据框，smote_data$class 是过采样后的标签
train_data_smote <- data.frame(smote_data$data, claim_status = smote_data$class)
train_data_smote$claim_status <- as.factor(train_data_smote$claim_status)

# 训练朴素贝叶斯模型
nb_model_smote <- naiveBayes(claim_status ~ ., data = train_data_smote)

# 使用测试集进行预测
test_data <- test_data %>%
  mutate(across(all_of(binary_features), as.numeric))
predictions_smote <- predict(nb_model_smote, newdata = test_data)

# 评估模型性能
confusion_matrix_smote <- table(test_data$claim_status, predictions_smote)
print(confusion_matrix_smote)

# 计算准确率
accuracy_smote <- sum(diag(confusion_matrix_smote)) / sum(confusion_matrix_smote)
print(paste("Accuracy (SMOTE): ", accuracy_smote))

# 计算精确率（Precision）、召回率（Recall）和 F1-score
library(caret)
confusion_matrix <- confusionMatrix(predictions_smote, test_data$claim_status)
print(confusion_matrix$byClass["Precision"])
print(confusion_matrix$byClass["Recall"])
print(confusion_matrix$byClass["F1"])


```


```{r}
install.packages("ROSE")
library(ROSE)

# 使用 ROSE 进行过采样
train_data_rose <- ROSE(claim_status ~ ., data = train_data, seed = 123)$data

# 训练朴素贝叶斯模型
nb_model_rose <- naiveBayes(claim_status ~ ., data = train_data_rose)

# 使用测试集进行预测
predictions_rose <- predict(nb_model_rose, newdata = test_data)

# 评估模型性能
confusion_matrix_rose <- table(test_data$claim_status, predictions_rose)
print(confusion_matrix_rose)

# 计算准确率
accuracy_rose <- sum(diag(confusion_matrix_rose)) / sum(confusion_matrix_rose)
print(paste("Accuracy (ROSE): ", accuracy_rose))

# 计算精确率（Precision）、召回率（Recall）和 F1-score
confusion_matrix <- confusionMatrix(predictions_rose, test_data$claim_status)
print(confusion_matrix$byClass["Precision"])
print(confusion_matrix$byClass["Recall"])
print(confusion_matrix$byClass["F1"])


```


```{r}
# 安装并加载必要的包
library(nnet)
library(caret)

# 转换二元特征为数值类型
car_data2[binary_features] <- lapply(car_data2[binary_features], function(x) ifelse(x == "Yes", 1, ifelse(x == "No", 0, x)))

# 选择二元特征数据
binary_data <- car_data2 %>% select(all_of(binary_features))

# 设置随机种子以保证结果可重复
set.seed(123)

# 拆分数据集为训练集和测试集
train_index <- sample(1:nrow(binary_data), 0.7 * nrow(binary_data))
train_data <- binary_data[train_index, ]
test_data <- binary_data[-train_index, ]

# 确保所有二元特征都是数值类型
train_data <- train_data %>%
  mutate(across(all_of(binary_features), as.numeric))
test_data <- test_data %>%
  mutate(across(all_of(binary_features), as.numeric))

# 训练神经网络模型
nn_model <- nnet(claim_status ~ ., data = train_data, size = 10, decay = 0.01, maxit = 200)

# 使用测试集进行预测（预测概率）
predictions_prob <- predict(nn_model, newdata = test_data, type = "raw")

# 将概率转换为类别（假设类别为 0 和 1）
predictions_nn <- ifelse(predictions_prob > 0.5, 1, 0)

# 评估模型性能
confusion_matrix_nn <- table(test_data$claim_status, predictions_nn)
print(confusion_matrix_nn)

# 计算准确率
accuracy_nn <- sum(diag(confusion_matrix_nn)) / sum(confusion_matrix_nn)
print(paste("Accuracy (Neural Network): ", accuracy_nn))

# 计算精确率（Precision）、召回率（Recall）和 F1-score
confusion_matrix <- confusionMatrix(as.factor(predictions_nn), as.factor(test_data$claim_status))
print(confusion_matrix$byClass["Precision"])
print(confusion_matrix$byClass["Recall"])
print(confusion_matrix$byClass["F1"])

```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```


```{r}


```


```{r}


```

```{r}


```

